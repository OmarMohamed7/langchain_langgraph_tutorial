# Complete Explanation: RAG, FAISS, and Similarity Mechanisms

## üìñ **Code Explanation (Step by Step)**

### **Step 1-2: Loading Documents**
```python
loader = TextLoader("docs.txt")
documents = loader.load()
```
- **Purpose**: Loads text from `docs.txt` file
- **Result**: Creates Document objects containing the text content

### **Step 3: Text Splitting (Tokenization)**
```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(documents)
```
- **Purpose**: Breaks large documents into smaller chunks
- **Why?**: LLMs have token limits. Chunks make it easier to:
  - Process large documents
  - Find relevant sections
  - Stay within model limits
- **Parameters**:
  - `chunk_size=1000`: Each chunk has ~1000 characters
  - `chunk_overlap=200`: Last 200 chars of one chunk = first 200 of next (prevents losing context at boundaries)

### **Step 4: Creating Embeddings**
```python
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(chunks, embeddings)
```
- **Embeddings**: Convert text ‚Üí numbers (vectors)
- **Why?**: Computers can't understand text, but can calculate with numbers
- **Example**: 
  - "AI is powerful" ‚Üí `[0.2, -0.5, 0.8, ...]` (384 numbers for this model)
  - Similar sentences ‚Üí Similar vectors
- **Model**: `all-MiniLM-L6-v2` creates 384-dimensional vectors

### **Step 5: Creating Retriever**
```python
retriever = vectorstore.as_retriever()
```
- **Purpose**: Creates a search interface
- **What it does**: When you ask a question, it finds the most relevant chunks

### **Step 6: LLM and Prompt Setup**
```python
model = ChatOllama(model="phi4-mini:latest")
prompt = ChatPromptTemplate.from_template(...)
```
- **Model**: The AI that generates answers
- **Prompt**: Instructions telling the model how to answer using the context

### **Step 7: Two Chains Explained** üéØ

#### **Why Two Chains?**

1. **Document Chain** (`create_stuff_documents_chain`):
   - **Purpose**: Takes documents + question ‚Üí generates answer
   - **Input**: `{context: "retrieved docs", input: "question"}`
   - **Output**: Answer from LLM
   - **What it does**: "Stuff" all retrieved documents into the prompt and ask the LLM

2. **Retrieval Chain** (`create_retrieval_chain`):
   - **Purpose**: Combines retrieval + document processing
   - **What it does**:
     - Takes your question
     - Uses retriever to find relevant chunks
     - Passes chunks + question to document chain
     - Returns final answer

**Flow**:
```
Question ‚Üí Retrieval Chain ‚Üí Retriever finds docs ‚Üí Document Chain ‚Üí LLM ‚Üí Answer
```

### **Step 8: Querying**
```python
response = retrieval_chain.invoke({"input": query})
```
- **What happens**:
  1. Your question is converted to an embedding
  2. FAISS finds similar document chunks
  3. Top chunks are sent to LLM with your question
  4. LLM generates answer based on those chunks

---

## üîç **FAISS Explained**

### **What is FAISS?**
- **FAISS** = Facebook AI Similarity Search
- **Purpose**: Fast similarity search in high-dimensional spaces
- **How it works**: Uses advanced indexing to find similar vectors quickly

### **Similarity Mechanism**

#### **1. Cosine Similarity** (Most Common)
- Measures angle between two vectors
- Range: -1 to 1
- **1.0** = Identical (same direction)
- **0.0** = Perpendicular (unrelated)
- **-1.0** = Opposite (opposite meaning)

**Example**:
```
Vector A: [1, 0, 0]
Vector B: [1, 0, 0]  ‚Üí Cosine = 1.0 (identical)

Vector A: [1, 0, 0]
Vector B: [0, 1, 0]  ‚Üí Cosine = 0.0 (perpendicular)

Vector A: [1, 0, 0]
Vector B: [-1, 0, 0] ‚Üí Cosine = -1.0 (opposite)
```

#### **2. Euclidean Distance** (L2)
- Measures straight-line distance between vectors
- Lower = More similar
- Range: 0 to ‚àû

#### **3. Dot Product**
- Multiplies corresponding elements and sums
- Higher = More similar (when vectors are normalized)

### **How FAISS Uses Similarity**

1. **Indexing Phase** (when you create vectorstore):
   - Converts all document chunks to embeddings
   - Builds an index for fast search
   - Stores vectors in optimized data structures

2. **Search Phase** (when you query):
   - Converts your question to embedding
   - Searches index for most similar vectors
   - Returns top-k most similar chunks

**Default in FAISS**: Uses **L2 (Euclidean) distance**, but can be configured for cosine similarity.

---

## üîÑ **Alternative Vector Stores (No Code Changes Needed!)**

You can swap FAISS with any of these by changing **ONE LINE**:

### **1. Chroma** (Recommended for Development)
```python
# Instead of: from langchain_community.vectorstores import FAISS
from langchain_chroma import Chroma

# Change this line:
vectorstore = Chroma.from_documents(chunks, embeddings)
# Everything else stays the same!
```
- **Pros**: Easy to use, good for prototyping, persistent storage
- **Install**: `pip install chromadb`

### **2. Pinecone** (Cloud, Production)
```python
from langchain_pinecone import PineconeVectorStore

vectorstore = PineconeVectorStore.from_documents(chunks, embeddings, index_name="my-index")
```
- **Pros**: Scalable, managed service, fast
- **Cons**: Requires API key, paid for large scale

### **3. Weaviate** (Self-hosted or Cloud)
```python
from langchain_weaviate import WeaviateVectorStore

vectorstore = WeaviateVectorStore.from_documents(chunks, embeddings)
```
- **Pros**: GraphQL interface, good for complex queries
- **Install**: `pip install weaviate-client`

### **4. Qdrant** (Open Source)
```python
from langchain_qdrant import QdrantVectorStore

vectorstore = QdrantVectorStore.from_documents(chunks, embeddings)
```
- **Pros**: Fast, good filtering capabilities
- **Install**: `pip install qdrant-client`

### **5. Milvus** (Scalable)
```python
from langchain_milvus import MilvusVectorStore

vectorstore = MilvusVectorStore.from_documents(chunks, embeddings)
```
- **Pros**: Handles billions of vectors, production-ready
- **Install**: `pip install pymilvus`

### **6. In-Memory (Simple Testing)**
```python
from langchain_community.vectorstores import InMemoryVectorStore

vectorstore = InMemoryVectorStore.from_documents(chunks, embeddings)
```
- **Pros**: No installation needed, instant
- **Cons**: Data lost when program ends

---

## üìä **Comparison Table**

| Vector Store | Similarity Metric | Speed | Scalability | Setup |
|--------------|------------------|-------|-------------|-------|
| **FAISS** | L2/Cosine | ‚ö°‚ö°‚ö° Very Fast | Medium | Easy |
| **Chroma** | Cosine | ‚ö°‚ö° Fast | Medium | Very Easy |
| **Pinecone** | Cosine/Euclidean | ‚ö°‚ö°‚ö° Very Fast | High | Medium |
| **Qdrant** | Cosine/Euclidean | ‚ö°‚ö°‚ö° Very Fast | High | Medium |
| **Weaviate** | Cosine | ‚ö°‚ö° Fast | High | Medium |
| **Milvus** | L2/IP | ‚ö°‚ö°‚ö° Very Fast | Very High | Complex |

---

## üéØ **Key Takeaways**

1. **Embeddings** convert text to numbers for similarity calculation
2. **FAISS** uses **L2 distance** by default (can use cosine)
3. **Similarity** = How "close" two vectors are in mathematical space
4. **Two chains** work together: Retrieval finds docs, Document chain generates answer
5. **You can swap vector stores** with minimal code changes - they all use the same interface!

---

## üîß **Changing Similarity Metric in FAISS**

If you want to use **cosine similarity** instead of L2:

```python
vectorstore = FAISS.from_documents(
    chunks, 
    embeddings,
    distance_strategy="COSINE"  # or "EUCLIDEAN_DISTANCE" (default)
)
```

That's it! The retriever will automatically use cosine similarity for search.
